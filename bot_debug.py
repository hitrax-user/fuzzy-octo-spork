
import logging
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
import os
import logging

log_path = os.path.join(os.path.dirname(__file__), "bot.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path, mode="a", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logging.info("üî• –õ–æ–≥–≥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ. –õ–æ–≥-—Ñ–∞–π–ª: %s", log_path)
print("üìÇ –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–∞:", os.getcwd())
print("üìÑ –û–∂–∏–¥–∞–µ–º—ã–π –ª–æ–≥-—Ñ–∞–π–ª:", log_path)


# Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name("credentials.json", scope)
client = gspread.authorize(creds)
sheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1OiUKuuJhHXNmTr-KWYdVl7UapIgAbDuuf9w34hbQNFU/edit#gid=0").sheet1

BOT_TOKEN = "7645593337:AAHWs1_kIdpUBZkdQxKd_OcN9IEzTC7umVs"

def parse_cian(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")

        title = soup.find("h1", {"data-mark": "title"}).text.strip()
        address = soup.find("span", {"data-mark": "address"}).text.strip()
        price_raw = soup.find("span", {"data-mark": "MainPrice"}).text.strip()
        price = re.sub(r"[^\d]", "", price_raw)

        details = soup.find_all("li", class_="a10a3f92e9--item--_ipjW")
        area = year = balcony = None

        area_text = next((d.text for d in details if "–º¬≤" in d.text), None)
        if area_text:
            match = re.search(r"\d+,\d+|\d+", area_text)
            area = float(match.group().replace(",", ".")) if match else None

        year_text = next((d.text for d in details if "–≥–æ–¥" in d.text.lower()), None)
        if year_text:
            match = re.search(r'\d{4}', year_text)
            if match:
                y = int(match.group())
                if 1800 <= y <= 2025:
                    year = y

        balcony_text = next((d.text for d in details if "–±–∞–ª–∫–æ–Ω" in d.text.lower()), None)
        if balcony_text:
            balcony_match = re.search(r"(–µ—Å—Ç—å|–Ω–µ—Ç)", balcony_text.lower())
            balcony = balcony_match.group(1) if balcony_match else balcony_text

        try:
            district = soup.find("span", {"data-mark": "district"}).text.strip()
        except:
            district = address.split(",")[1].strip() if "," in address else None

        logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏ Avito: {url}")
        return {
            "title": title,
            "address": address,
            "district": district,
            "area": area,
            "year": year,
            "price": price,
            "url": url,
            "balcony": balcony
        }

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ CIAN –¥–ª—è {url}: {e}")
        return None


# –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è parse_avito —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º


def parse_avito(url):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--window-size=1920,1080")
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)

    driver = None
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        title = address = price = district = balcony = None
        area = year = None

        try:
            title_raw = driver.title
            title_match = re.match(r"^(.*?)\s*\|", title_raw)
            if title_match:
                title = title_match.group(1).strip()
        except:
            pass

        try:
            address = driver.find_element(By.CLASS_NAME, "style-item-address__string-wt61A").text
        except Exception as e:
            logging.warning(f"–ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")

        try:
            price_element = driver.find_element(By.CSS_SELECTOR, '[itemprop="price"]')
            price_raw = price_element.text
            price = re.sub(r"[^\d]", "", price_raw)
        except:
            try:
                price_raw = driver.find_element(By.CLASS_NAME, "js-item-price").text
                price = re.sub(r"[^\d]", "", price_raw)
            except Exception as e:
                logging.warning(f"–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {e}")

        try:
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.params-paramsList > li")))
            all_params = driver.find_elements(By.CSS_SELECTOR, "ul.params-paramsList > li")
            for item in all_params:
                text = item.text
                logging.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä: {text}")
                if "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å" in text:
                    match = re.search(r"\d+,\d+|\d+", text)
                    if match:
                        area = float(match.group().replace(",", "."))
                elif "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏" in text or "–ì–æ–¥ —Å–¥–∞—á–∏" in text:
                    match = re.search(r'\d{4}', text)
                    if match:
                        y = int(match.group())
                        if 1800 <= y <= 2025:
                            year = y
                elif "–ë–∞–ª–∫–æ–Ω" in text:
                    if "–Ω–µ—Ç" in text.lower():
                        balcony = "–Ω–µ—Ç"
                    elif "–µ—Å—Ç—å" in text.lower() or re.search(r"\d+", text):
                        balcony = "–µ—Å—Ç—å"
                    else:
                        balcony = text.split(":")[1].strip()
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")

        if address and "," in address:
            parts = address.split(",")
            if len(parts) > 1:
                district = parts[1].strip()

        logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏ Avito: {url}")
        return {
            "title": title,
            "address": address,
            "district": district,
            "area": area,
            "year": year,
            "price": price,
            "url": url,
            "balcony": balcony
        }

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Avito –¥–ª—è {url}: {e}")
        return None
    finally:
        if driver:
            driver.quit()



async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    url = update.message.text.strip()
    logging.info("–ë–æ—Ç –ø–æ–ª—É—á–∏–ª —Å—Å—ã–ª–∫—É: " + url)  # <-- –≤—Å—Ç–∞–≤—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É
    domain = urlparse(url).netloc

    if not any(s in domain for s in ["cian.ru", "avito.ru"]):
        await update.message.reply_text("‚õî –¢–æ–ª—å–∫–æ CIAN –∏ Avito.")
        return

    try:
        existing_urls = [row[7] for row in sheet.get_all_values()[1:]]
        if url in existing_urls:
            await update.message.reply_text("‚ö†Ô∏è –£–∂–µ –≤ —Ç–∞–±–ª–∏—Ü–µ.")
            return
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")

    data = parse_cian(url) if "cian.ru" in domain else parse_avito(url)

    if not data:
        await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫—É.")
        return

    row = [
        len(sheet.get_all_values()) + 1,
        data["title"] or "",
        data["address"] or "",
        data["district"] or "",
        str(data["area"]) if data["area"] is not None else "",
        str(data["year"]) if data["year"] is not None else "",
        data["price"] or "",
        data["url"],
        data["balcony"] or "",
        ""
    ]

    try:
        sheet.append_row(row)
        await update.message.reply_text(
            f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü—É!\n"
            f"üìù –ù–∞–∑–≤–∞–Ω–∏–µ: {data['title'] or '‚Äî'}\n"
            f"üè° –ê–¥—Ä–µ—Å: {data['address'] or '‚Äî'}\n"
            f"üìç –†–∞–π–æ–Ω: {data['district'] or '‚Äî'}\n"
            f"üìê –ü–ª–æ—â–∞–¥—å: {data['area']} –º¬≤\n"
            f"üï∞ –ì–æ–¥: {data['year']}\n"
            f"üí∞ –¶–µ–Ω–∞: {data['price']} ‚ÇΩ\n"
            f"ü™ü –ë–∞–ª–∫–æ–Ω: {data['balcony'] or '‚Äî'}"
        )
    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É –¥–ª—è {url}: {e}")
        await update.message.reply_text("üö® –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É.")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –ö–∏–¥–∞–π —Å—Å—ã–ª–∫—É –Ω–∞ Avito –∏–ª–∏ CIAN ‚Äî —è –¥–æ–±–∞–≤–ª—é –µ—ë –≤ —Ç–∞–±–ª–∏—Ü—É üìã")

def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.run_polling()

if __name__ == "__main__":
    main()
