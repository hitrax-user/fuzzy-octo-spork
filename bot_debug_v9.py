
import logging
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
import os
import logging

log_path = os.path.join(os.path.dirname(__file__), "bot.log")

logging.basicConfig(
       level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path, mode="a", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logging.info("üî• –õ–æ–≥–≥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ. –õ–æ–≥-—Ñ–∞–π–ª: %s", log_path)
print("üìÇ –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–∞:", os.getcwd())
print("üìÑ –û–∂–∏–¥–∞–µ–º—ã–π –ª–æ–≥-—Ñ–∞–π–ª:", log_path)


# Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name("credentials.json", scope)
client = gspread.authorize(creds)
sheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1OiUKuuJhHXNmTr-KWYdVl7UapIgAbDuuf9w34hbQNFU/edit#gid=0").sheet1

BOT_TOKEN = "7645593337:AAHWs1_kIdpUBZkdQxKd_OcN9IEzTC7umVs"

def parse_cian(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")

        title = soup.find("h1", {"data-mark": "title"}).text.strip()
        address = soup.find("span", {"data-mark": "address"}).text.strip()
        price_raw = soup.find("span", {"data-mark": "MainPrice"}).text.strip()
        price = re.sub(r"[^\d]", "", price_raw)

        details = soup.find_all("li", class_="a10a3f92e9--item--_ipjW")
        area = year = balcony = None

        area_text = next((d.text for d in details if "–º¬≤" in d.text), None)
        if area_text:
            match = re.search(r"\d+,\d+|\d+", area_text)
            area = float(match.group().replace(",", ".")) if match else None

        year_text = next((d.text for d in details if "–≥–æ–¥" in d.text.lower()), None)
        if year_text:
            match = re.search(r'\d{4}', year_text)
            if match:
                y = int(match.group())
                if 1800 <= y <= 2025:
                    year = y

        balcony_text = next((d.text for d in details if "–±–∞–ª–∫–æ–Ω" in d.text.lower()), None)
        if balcony_text:
            balcony_match = re.search(r"(–µ—Å—Ç—å|–Ω–µ—Ç)", balcony_text.lower())
            balcony = balcony_match.group(1) if balcony_match else balcony_text

        try:
            district = soup.find("span", {"data-mark": "district"}).text.strip()
        except:
            district = address.split(",")[1].strip() if "," in address else None

        logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏ Avito: {url}")
        return {
            "title": title,
            "address": address,
            "district": district,
            "area": area,
            "year": year,
            "price": price,
            "url": url,
            "balcony": balcony
        }

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ CIAN –¥–ª—è {url}: {e}")
        return None


# –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è parse_avito —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º



def parse_avito(url):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--window-size=1920,1080")
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)

    driver = None
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        title = address = price = district = balcony = None
        area = year = None

        try:
            title_raw = driver.title
            title_match = re.match(r"^(.*?)\s*\|", title_raw)
            if title_match:
                title = title_match.group(1).strip()
        except:
            pass

        try:
            address = driver.find_element(By.CLASS_NAME, "style-item-address__string-wt61A").text
        except Exception as e:
            logging.warning(f"–ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")

        try:
            price_element = driver.find_element(By.CSS_SELECTOR, '[itemprop="price"]')
            price = price_element.get_attribute("content")
        except:
            try:
                price_raw = driver.find_element(By.CLASS_NAME, "js-item-price").text
                price = re.sub(r"[^\d]", "", price_raw)
            except Exception as e:
                logging.warning(f"–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {e}")

        try:
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".params-paramsList__item--2Y20")))
            all_params = driver.find_elements(By.CSS_SELECTOR, ".params-paramsList__item--2Y20")
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –æ–∂–∏–¥–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
            all_params = []

        if all_params:
            for item in all_params:
                text = item.text
                logging.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä: {text}")
                if "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å" in text:
                    match = re.search(r"\d+,\d+|\d+", text)
                    if match:
                        area = float(match.group().replace(",", "."))
                elif "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏" in text or "–ì–æ–¥ —Å–¥–∞—á–∏" in text:
                    match = re.search(r'\d{4}', text)
                    if match:
                        y = int(match.group())
                        if 1800 <= y <= 2025:
                            year = y
                elif "–ë–∞–ª–∫–æ–Ω" in text:
                    if "–Ω–µ—Ç" in text.lower():
                        balcony = "–Ω–µ—Ç"
                    elif "–µ—Å—Ç—å" in text.lower() or re.search(r"\d+", text):
                        balcony = "–µ—Å—Ç—å"
                    else:
                        parts = text.split(":")
                        if len(parts) > 1:
                            balcony = parts[1].strip()
        else:
            logging.warning("‚ùó –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á–µ—Ä–µ–∑ Selenium, –ø—Ä–æ–±—É–µ–º BeautifulSoup")
            try:
                soup = BeautifulSoup(driver.page_source, "html.parser")
                param_items = soup.find_all("li", class_=re.compile("params-paramsList.*"))
                for item in param_items:
                    text = item.get_text(strip=True)
                    logging.info(f"[BS] –ü–∞—Ä–∞–º–µ—Ç—Ä: {text}")
                    if "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å" in text:
                        match = re.search(r"\d+,\d+|\d+", text)
                        if match:
                            area = float(match.group().replace(",", "."))
                    elif "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏" in text or "–ì–æ–¥ —Å–¥–∞—á–∏" in text:
                        match = re.search(r'\d{4}', text)
                        if match:
                            y = int(match.group())
                            if 1800 <= y <= 2025:
                                year = y
                    elif "–ë–∞–ª–∫–æ–Ω" in text:
                        if "–Ω–µ—Ç" in text.lower():
                            balcony = "–Ω–µ—Ç"
                        elif "–µ—Å—Ç—å" in text.lower() or re.search(r"\d+", text):
                            balcony = "–µ—Å—Ç—å"
                        else:
                            parts = text.split(":")
                            if len(parts) > 1:
                                balcony = parts[1].strip()
            except Exception as e_bs:
                logging.warning(f"BeautifulSoup –Ω–µ –ø–æ–º–æ–≥: {e_bs}")

        if address and "," in address:
            parts = address.split(",")
            if len(parts) > 1:
                district = parts[1].strip()

        return {
            "title": title,
            "address": address,
            "district": district,
            "area": area,
            "year": year,
            "price": price,
            "url": url,
            "balcony": balcony
        }

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Avito –¥–ª—è {url}: {e}")
        return None
    finally:
        if driver:
            driver.quit()

    driver = None
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        title = address = price = district = balcony = None
        area = year = None

        try:
            title_raw = driver.title
            title_match = re.match(r"^(.*?)\s*\|", title_raw)
            if title_match:
                title = title_match.group(1).strip()
        except:
            pass

        try:
            address = driver.find_element(By.CLASS_NAME, "style-item-address__string-wt61A").text
        except Exception as e:
            logging.warning(f"–ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")

        try:
            price_element = driver.find_element(By.CSS_SELECTOR, '[itemprop="price"]')
            price = price_element.get_attribute("content")
        except:
            try:
                price_raw = driver.find_element(By.CLASS_NAME, "js-item-price").text
                price = re.sub(r"[^\d]", "", price_raw)
            except Exception as e:
                logging.warning(f"–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {e}")

        try:
            wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "params-paramsList__item--2Y20")))
            all_params = driver.find_elements(By.CLASS_NAME, "params-paramsList__item--2Y20")
            for item in all_params:
                text = item.text.replace("\xa0", " ")
                logging.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä: {text}")
                if "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å" in text:
                    match = re.search(r"\d+,\d+|\d+", text)
                    if match:
                        area = float(match.group().replace(",", "."))
                elif "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏" in text or "–ì–æ–¥ —Å–¥–∞—á–∏" in text:
                    match = re.search(r'\d{4}', text)
                    if match:
                        y = int(match.group())
                        if 1800 <= y <= 2025:
                            year = y
                elif "–ë–∞–ª–∫–æ–Ω" in text:
                    if "–Ω–µ—Ç" in text.lower():
                        balcony = "–Ω–µ—Ç"
                    elif "–µ—Å—Ç—å" in text.lower() or re.search(r"\d+", text):
                        balcony = "–µ—Å—Ç—å"
                    else:
                        parts = text.split(":")
                        if len(parts) > 1:
                            balcony = parts[1].strip()
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")

        if address and "," in address:
            parts = address.split(",")
            if len(parts) > 1:
                district = parts[1].strip()

        logging.info(f"‚úÖ –†–∞—Å–ø–∞—Ä—Å–∏–ª–∏ Avito:\nüìå {title}\nüè† {address}, {district}\nüìê {area} –º¬≤, üèó {year}, ü™ü –ë–∞–ª–∫–æ–Ω: {balcony}\nüí∞ {price} ‚ÇΩ")

        return {
            "title": title,
            "address": address,
            "district": district,
            "area": area,
            "year": year,
            "price": price,
            "url": url,
            "balcony": balcony
        }

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Avito –¥–ª—è {url}: {e}")
        return None
    finally:
        if driver:
            driver.quit()




async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å —Å—Å—ã–ª–∫—É –Ω–∞ Avito –∏–ª–∏ CIAN, –∏ —è –¥–æ–±–∞–≤–ª—é –µ—ë –≤ —Ç–∞–±–ª–∏—Ü—É üìÑ")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    url = update.message.text.strip()
    logging.info("–ë–æ—Ç –ø–æ–ª—É—á–∏–ª —Å—Å—ã–ª–∫—É: " + url)

    if "avito.ru" in url:
        data = parse_avito(url)
    elif "cian.ru" in url:
        data = parse_cian(url)
    else:
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∏—à–ª–∏ —Å—Å—ã–ª–∫—É —Å Avito –∏–ª–∏ CIAN üôè")
        return

    if not data:
        await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫—É.")
        return

    message = (
        f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü—É!\n"
        f"üìù –ù–∞–∑–≤–∞–Ω–∏–µ: {data['title']}\n"
        f"üè° –ê–¥—Ä–µ—Å: {data['address']}\n"
        f"üìç –†–∞–π–æ–Ω: {data['district']}\n"
        f"üìê –ü–ª–æ—â–∞–¥—å: {data['area']} –º¬≤\n"
        f"üï∞ –ì–æ–¥: {data['year']}\n"
        f"üí∞ –¶–µ–Ω–∞: {data['price']} ‚ÇΩ\n"
        f"ü™ü –ë–∞–ª–∫–æ–Ω: {data['balcony']}"
    )

    await update.message.reply_text(message)

    try:
        sheet.append_row([
            data["title"],
            data["address"],
            data["district"],
            data["area"],
            data["year"],
            data["price"],
            data["url"],
            data["balcony"],
            ""  # –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        ])
    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É –¥–ª—è {url}: {e}")


def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.run_polling()

if __name__ == "__main__":
    main()
